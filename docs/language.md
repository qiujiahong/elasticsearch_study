# 分词器

从一串文本中切分出一个一个词条，并对每个词条做标准化。  
包括三部分：
* character filter: 分词前预处理，过滤掉html标签，特殊符号等；
* tokenizer: 分词
* token filter: 标准化

内置分词器： 
* standard
* simple
* whitespace
* language




